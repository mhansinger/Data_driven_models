{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd07b03001a3ba4464a0cd333db0c46258603845fbd8d493c04cbac40e067ccfe9d",
   "display_name": "Python 3.7.6 64-bit ('tf_1.14': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "K.clear_session()\n",
    "\n",
    "from keras import Input, optimizers, layers, losses\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Input, BatchNormalization, concatenate, ReLU, LeakyReLU, Dropout\n",
    "from keras.callbacks import CSVLogger, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=pd.read_pickle('./pickle/PF_decay_train_reduced_stratified_no_log.pkl')\n",
    "data_val=pd.read_pickle('./pickle/PF_decay_val_reduced_stratified_no_log.pkl')\n",
    "data_mean_std=pd.read_csv('./data_mean_std.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_input=['c_tilde', 'abs_grad_c_tilde', 'c_var', 'mag_vel', 'mag_grad_vel', 'UP_delta', 'mag_strain', 'mag_vor', 'kappa_LES', 'a_T_LES']\n",
    "\n",
    "X = data_train[list_input]\n",
    "C = data_train[['Delta_del_th']]\n",
    "Y = data_train[['omega_LES']]\n",
    "\n",
    "X_val=data_val[list_input]\n",
    "C_val=data_val[['Delta_del_th']]\n",
    "Y_val=data_val[['omega_LES']]\n",
    "\n",
    "dataX_row_count = X.shape[0]\n",
    "dataX_column_count = len(X.columns)\n",
    "dataY_column_count = len(Y.columns)\n",
    "noise_size = 1\n",
    "condition_size = 1\n",
    "\n",
    "lr_disc=1e-4\n",
    "lr_gen=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSGAN():\n",
    "    def __init__(self):                   \n",
    "        self.generator = self.build_generator()\n",
    "        self.discriminator = self.build_discriminator()\n",
    "\n",
    "        noise     = Input(shape=(noise_size,))\n",
    "        x_arr     = Input(shape=(dataX_column_count,))\n",
    "        condition = Input(shape=(condition_size,))\n",
    "\n",
    "        fake_arr = self.generator([noise, condition, x_arr])\n",
    "        valid = self.discriminator([x_arr, condition, fake_arr])\n",
    "       \n",
    "        self.discriminator.compile(optimizer=optimizers.Adam(learning_rate=lr_disc, beta_1=0.5, beta_2=0.999),\n",
    "                                   loss=['mse'])\n",
    "\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        self.combined = Model([noise, condition, x_arr], valid)\n",
    "        self.combined.compile(optimizer=optimizers.Adam(learning_rate=lr_gen, beta_1=0.5, beta_2=0.999),\n",
    "                              loss=['mse'])\n",
    "\n",
    "    def element(self, input_tensor, n_nodes, activation):\n",
    "        x = Dense(n_nodes)(input_tensor)\n",
    "        # x = BatchNormalization()(x)\n",
    "\n",
    "        if activation==0:\n",
    "            x = ReLU()(x)\n",
    "        elif activation==1:\n",
    "            x = LeakyReLU()(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def res_block(self, input_tensor, n_nodes, activation):\n",
    "        x = self.element(input_tensor, n_nodes, activation)\n",
    "        x = self.element(x, n_nodes, activation)\n",
    "        x = self.element(x, n_nodes, activation)\n",
    "    \n",
    "        x = layers.add([x, input_tensor])\n",
    "        if activation==0:\n",
    "            x = ReLU()(x)\n",
    "        elif activation==1:\n",
    "            x = LeakyReLU()(x)\n",
    "    \n",
    "        return x\n",
    "\n",
    "    def build_generator(self,\n",
    "        n_nodes=100,\n",
    "        n_res_block_gen=3):\n",
    "\n",
    "        input_x  = Input(shape=(dataX_column_count,))\n",
    "        output_x = self.element(input_x, n_nodes, 0)\n",
    "        output_x = self.element(output_x, n_nodes, 0)\n",
    "\n",
    "        input_noise  = Input(shape=(noise_size,))\n",
    "        output_noise = self.element(input_noise, 25, 0)\n",
    "        output_noise = self.element(output_noise, 25, 0)\n",
    "\n",
    "        input_condition  = Input(shape=(condition_size,))\n",
    "        output_condition = self.element(input_condition, 25, 0)\n",
    "        output_condition = self.element(output_condition, 25, 0)\n",
    "\n",
    "        x = concatenate([output_x, output_noise, output_condition])\n",
    "        x = self.element(x, n_nodes, 0)\n",
    "        for _ in range(n_res_block_gen):\n",
    "            x = self.res_block(x, n_nodes, 0)\n",
    "\n",
    "        output = Dense(dataY_column_count, activation='linear')(x)\n",
    "\n",
    "        model = Model(inputs=[input_noise, input_condition, input_x], outputs=output)\n",
    "\n",
    "        return(model)\n",
    "\n",
    "    def build_discriminator(self,\n",
    "        n_nodes=100,\n",
    "        n_res_block_crit=3):\n",
    "\n",
    "        input_x = Input(shape=(dataX_column_count,))\n",
    "        input_condition  = Input(shape=(condition_size,))\n",
    "        input_label = Input(shape=(dataY_column_count,))\n",
    "\n",
    "        x = concatenate([input_x, input_condition, input_label])\n",
    "                \n",
    "        x = self.element(x, n_nodes, 1)\n",
    "        for _ in range(n_res_block_crit):\n",
    "            x = self.res_block(x, n_nodes, 1)\n",
    "\n",
    "        loss = Dense(1)(x)\n",
    "\n",
    "        model = Model(inputs=[input_x, input_condition, input_label], outputs=loss)\n",
    "    \n",
    "        return(model)\n",
    "\n",
    "    def index_sweep(self, idx, size, tot):\n",
    "        start=idx*size\n",
    "        end=start+size\n",
    "\n",
    "        if idx==tot:\n",
    "            end=tot\n",
    "\n",
    "        return(range(start,end))\n",
    "\n",
    "    def predict(self, xtest, ctest):\n",
    "        noise = np.random.normal(0, 1, (xtest.shape[0], noise_size))\n",
    "        ypred = self.generator.predict([noise, ctest, xtest])\n",
    "        return ypred\n",
    "    \n",
    "    def inverse_log(self, val, mean, std):\n",
    "        return np.exp(val * std + mean)\n",
    "\n",
    "    def inverse(self, val, mean, std):\n",
    "        return val * std + mean\n",
    "\n",
    "    def train(self, xtrain, ctrain, ytrain, epochs, batch_size, verbose=True, show=True, metric_val=True):\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake  = np.zeros((batch_size, 1))\n",
    "\n",
    "        dLossReal = np.zeros([epochs, 1])\n",
    "        dLossFake = np.zeros([epochs, 1])\n",
    "        dLoss = np.zeros([epochs, 1])\n",
    "        gLoss = np.zeros([epochs, 1])\n",
    "\n",
    "        mse_val = np.zeros([epochs])\n",
    "        r2_val = np.zeros([epochs])\n",
    "        js_val = np.zeros([epochs])\n",
    "\n",
    "        mse_test = np.zeros([epochs])\n",
    "        r2_test = np.zeros([epochs])\n",
    "        js_test = np.zeros([epochs])\n",
    "\n",
    "        header=pd.DataFrame(np.array([['epoch', 'd_loss_real', 'd_loss_fake', 'g_loss', 'mse_val', 'r2_val', 'js_val', 'mse_test', 'r2_test', 'js_test']]))\n",
    "        header.to_csv('./history.csv', mode='w', header=False, index=False)\n",
    "\n",
    "        Y_val_inverse = self.inverse(Y_val, data_mean_std.loc['omega_LES']['mean'], data_mean_std.loc['omega_LES']['std'])\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            t0=timer()\n",
    "            no_batch=int(xtrain.shape[0] // batch_size)\n",
    "\n",
    "            for batch_idx in tqdm(range(no_batch)):\n",
    "                idx_sweep = self.index_sweep(batch_idx,batch_size,no_batch)\n",
    "                idx_ran = np.random.randint(0, xtrain.shape[0], batch_size)\n",
    "                \n",
    "                x, c, true_labels = xtrain.iloc[idx_sweep], ctrain.iloc[idx_sweep], ytrain.iloc[idx_sweep]\n",
    "                noise = np.random.normal(0, 1, (batch_size, noise_size))\n",
    "                \n",
    "                fake_labels = self.generator.predict([noise, c, x])\n",
    "                \n",
    "                d_lossreal = self.discriminator.train_on_batch([x, c, true_labels], valid)\n",
    "                d_lossfake = self.discriminator.train_on_batch([x, c, fake_labels], fake)\n",
    "                d_loss = 0.5 * np.add(d_lossreal, d_lossfake)\n",
    "\n",
    "                g_loss = self.combined.train_on_batch([noise, c, x], valid)\n",
    "\n",
    "            dLossReal[epoch,0] = d_lossreal\n",
    "            dLossFake[epoch,0] = d_lossfake\n",
    "            dLoss[epoch,0] = d_loss\n",
    "            gLoss[epoch,0] = g_loss\n",
    "        \n",
    "            t1=timer()\n",
    "\n",
    "            if verbose:\n",
    "                print('\\n')\n",
    "                print(f'Epoch: {epoch} / dLoss: {d_loss} / gLoss: {g_loss} / Time: {t1-t0} sec')\n",
    "        \n",
    "            if show:\n",
    "                n=np.unique(data_val['n_Delta']).shape[0]\n",
    "                fig, axes = plt.subplots(2, n, figsize=(25,10), sharey=True)\n",
    "\n",
    "                for i in range(n):\n",
    "                    Delta = np.unique(data_val['n_Delta'])[i]\n",
    "                    X_pred = X_val[data_val['n_Delta']==Delta].sample(n=1000, random_state=0)                    \n",
    "                    C_pred = C_val[data_val['n_Delta']==Delta].sample(n=1000, random_state=0)\n",
    "                    Y_true = Y_val[data_val['n_Delta']==Delta].sample(n=1000, random_state=0)\n",
    "\n",
    "                    Y_pred = self.predict(X_pred, C_pred)                    \n",
    "\n",
    "                    axes[0,i].scatter(self.inverse(X_pred['c_tilde'], data_mean_std.loc['c_tilde']['mean'], data_mean_std.loc['c_tilde']['std']),\n",
    "                                      self.inverse(Y_true, data_mean_std.loc['omega_LES']['mean'], data_mean_std.loc['omega_LES']['std']),\n",
    "                                      s=1.5)\n",
    "\n",
    "                    axes[0,i].scatter(self.inverse(X_pred['c_tilde'], data_mean_std.loc['c_tilde']['mean'], data_mean_std.loc['c_tilde']['std']),\n",
    "                                      self.inverse(Y_pred, data_mean_std.loc['omega_LES']['mean'], data_mean_std.loc['omega_LES']['std']),\n",
    "                                      s=1.5)\n",
    "\n",
    "                    axes[1,i].scatter(self.inverse(Y_true, data_mean_std.loc['omega_LES']['mean'], data_mean_std.loc['omega_LES']['std']),\n",
    "                                      self.inverse(Y_pred, data_mean_std.loc['omega_LES']['mean'], data_mean_std.loc['omega_LES']['std']),\n",
    "                                      s=1.5)\n",
    "\n",
    "                plt.savefig(f'Inf_Val_Epoch_{epoch}.jpg', dpi=200, box_inches='tight')\n",
    "\n",
    "            if metric_val:\n",
    "                y_pred_val=self.predict(X_val, C_val)\n",
    "                y_pred_val=self.inverse(y_pred_val, data_mean_std.loc['omega_LES']['mean'], data_mean_std.loc['omega_LES']['std'])\n",
    "                y_pred_val[y_pred_val<0]=1e-8\n",
    "\n",
    "                mse_val[epoch]=mean_squared_error(y_pred_val, Y_val_inverse)\n",
    "                r2_val[epoch]=r2_score(y_pred_val, Y_val_inverse)\n",
    "                js_val[epoch]=jensenshannon(y_pred_val, Y_val_inverse)\n",
    "\n",
    "                print('\\n')\n",
    "                print('MSE_Val: {}, R2_Val: {}, JS_Val: {}'.format(mse_val[epoch], r2_val[epoch], js_val[epoch]))\n",
    "\n",
    "                history=pd.DataFrame(np.array([[epoch, dLossReal[epoch,0], dLossFake[epoch,0], gLoss[epoch,0], mse_val[epoch], r2_val[epoch], js_val[epoch]]]))\n",
    "                history.to_csv('./history.csv', mode='a', header=False, index=False)\n",
    "\n",
    "        return dLossReal, dLossFake, dLoss, gLoss, mse_val, r2_val, js_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsgan = LSGAN()\n",
    "dLossReal, dLossFake, dLoss, gLoss, mse_val, r2_val, js_val = lsgan.train(X, C, Y, 100, 2**10, metric_val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}